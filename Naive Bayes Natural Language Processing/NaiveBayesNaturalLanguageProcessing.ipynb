{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd84f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint \n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9da48",
   "metadata": {},
   "source": [
    "**WARNING: Offensive Language in the Data**\n",
    "\n",
    "## Multinomial Naive Bayes vs Offensive Tweets\n",
    "\n",
    "I'll use Multinomial Naive Bayes to help classify this dataset's tweets into either hate speech, simply offensive language, or neither.\n",
    "### Attributes\n",
    "For simplicity's sake, I'm going to drop all columns except \"class\" and \"tweet.\" This dataset originally had people manually read through the tweet text, and then vote on whether the tweet was offensive, hate speech, or neither. Hopefully Naive Bayes and NLTK can do just as well!\n",
    "- Levels for **class**:\n",
    "    1. 0 = Hate Speech\n",
    "    2. 1 = Offensive Language\n",
    "    3. 2 = Neither\n",
    "    \n",
    "So, our classifier needs to discern between 3 distinct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49cecadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate = pd.read_csv(\"hate_speech.csv\")\n",
    "hate.drop(columns = ['Unnamed: 0', 'count', 'hate_speech', 'neither', 'offensive_language'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c2db5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                              tweet\n",
       "0          2  !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1          1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2          1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3          1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4          1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
       "...      ...                                                ...\n",
       "24778      1  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
       "24779      2  you've gone and broke the wrong heart baby, an...\n",
       "24780      1  young buck wanna eat!!.. dat nigguh like I ain...\n",
       "24781      1              youu got wild bitches tellin you lies\n",
       "24782      2  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
       "\n",
       "[24783 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2a7f0",
   "metadata": {},
   "source": [
    "### Sample of uncleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63169d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215    \"@ProdsJewel_: &#8220;@sreadthepeace: &#8220;@...\n",
       "216    \"@QUAN1T0: 61% of welfare/government aid is cl...\n",
       "217    \"@QUAN1T0: These bitches don't care they just ...\n",
       "218    \"@Queen_Kaaat: It took a while for you to find...\n",
       "219    \"@RTNBA: Drakes new shoes that will be release...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate.iloc[215:220]['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75346c97",
   "metadata": {},
   "source": [
    "### Replace @user from tweets, and http links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68f5e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215    \": &#8220;: &#8220;:  Prince, THIS is art. htt...\n",
       "216    \": 61% of welfare/government aid is claimed by...\n",
       "217    \": These bitches don't care they just play tha...\n",
       "218    \": It took a while for you to find me, but I w...\n",
       "219    \": Drakes new shoes that will be released by N...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate['tweet'] = hate['tweet'].replace('@\\w*', '', regex = True)\n",
    "hate.iloc[215:220]['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152afd68",
   "metadata": {},
   "source": [
    "### Cleaning the text\n",
    "I ran the text through the same cleaning function I made before, which essentially removes punctuation and tokenizes words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67e91ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_strings = []\n",
    "for i in range(len(hate.values)):\n",
    "    cleaned_strings.append(' '.join(clean(hate.iloc[i].tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57764242",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate['tweet'] = cleaned_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e1855",
   "metadata": {},
   "source": [
    "### Cleaned samples (well, cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd2e090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215                   prince art http nobody takin bitch\n",
       "216         aid claimed white people black slander trash\n",
       "217                               bitches care play role\n",
       "218    took find hiding lime tree dis bitch da wrong ...\n",
       "219    drakes new shoes released yes glitter shoes ht...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate.iloc[215:220]['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cabe70",
   "metadata": {},
   "source": [
    "## Simple Model\n",
    "### tfid transforming, train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a63ed111",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfid.fit_transform(hate.tweet)\n",
    "y = hate['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba03fa4",
   "metadata": {},
   "source": [
    "### Untuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e3ce471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7994620040349697\n"
     ]
    }
   ],
   "source": [
    "mnb.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", mnb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a4544",
   "metadata": {},
   "source": [
    "### Tuning alpha and ngram ranges\n",
    "For the same reasons as before, I could not run this through a pipeline. This for loop accomplishes the same thing, more or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c51db00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Accuracy': 0.8375508264579855, 'ngram_range': (1, 1), 'nb_alpha': 0.4},\n",
       " {'Accuracy': 0.826777424310943, 'ngram_range': (1, 1), 'nb_alpha': 0.6},\n",
       " {'Accuracy': 0.8168512518879328, 'ngram_range': (1, 1), 'nb_alpha': 0.8},\n",
       " {'Accuracy': 0.8088618650090948, 'ngram_range': (1, 1), 'nb_alpha': 1.0},\n",
       " {'Accuracy': 0.8224604840466212, 'ngram_range': (1, 2), 'nb_alpha': 0.4},\n",
       " {'Accuracy': 0.8010745630528074, 'ngram_range': (1, 2), 'nb_alpha': 0.6},\n",
       " {'Accuracy': 0.7891708755483104, 'ngram_range': (1, 2), 'nb_alpha': 0.8},\n",
       " {'Accuracy': 0.7835619201290844, 'ngram_range': (1, 2), 'nb_alpha': 1.0},\n",
       " {'Accuracy': 0.8135433362602601, 'ngram_range': (2, 2), 'nb_alpha': 0.4},\n",
       " {'Accuracy': 0.7907446765908694, 'ngram_range': (2, 2), 'nb_alpha': 0.6},\n",
       " {'Accuracy': 0.7812214693823598, 'ngram_range': (2, 2), 'nb_alpha': 0.8},\n",
       " {'Accuracy': 0.77775115652677, 'ngram_range': (2, 2), 'nb_alpha': 1.0},\n",
       " {'Accuracy': 0.8066836487023685, 'ngram_range': (2, 3), 'nb_alpha': 0.4},\n",
       " {'Accuracy': 0.782512865943172, 'ngram_range': (2, 3), 'nb_alpha': 0.6},\n",
       " {'Accuracy': 0.7781142712587185, 'ngram_range': (2, 3), 'nb_alpha': 0.8},\n",
       " {'Accuracy': 0.7766212455815289, 'ngram_range': (2, 3), 'nb_alpha': 1.0},\n",
       " {'Accuracy': 0.7790021165685219, 'ngram_range': (3, 3), 'nb_alpha': 0.4},\n",
       " {'Accuracy': 0.777065103121263, 'ngram_range': (3, 3), 'nb_alpha': 0.6},\n",
       " {'Accuracy': 0.7758545144834039, 'ngram_range': (3, 3), 'nb_alpha': 0.8},\n",
       " {'Accuracy': 0.775047428260178, 'ngram_range': (3, 3), 'nb_alpha': 1.0}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_ranges = [(1,1), (1,2), (2, 2), (2, 3), (3,3)]\n",
    "alpha = [.4, .6, .8, 1.0]\n",
    "scores = []\n",
    "for i in ngram_ranges:\n",
    "    tfid = TfidfVectorizer(ngram_range = i)\n",
    "    X = tfid.fit_transform(hate.tweet)\n",
    "    for j in alpha:\n",
    "        score = cross_val_score(nb.MultinomialNB(alpha = j), X, y, cv = 10)\n",
    "        scores.append({\"Accuracy\" : np.mean(score),\n",
    "                       \"ngram_range\" : i,\n",
    "                       \"nb_alpha\" : j\n",
    "                      })\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73f067",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "According to the cv search, the best parameters look to be ngram_range set to (1,1) and the smoothing parameter (alpha) for the MultinomialNB set to .4. Let's see how it looks for our test set now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4519b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8314727639542704\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(ngram_range = (1,1))\n",
    "X = tfid.fit_transform(hate.tweet)\n",
    "y = hate['class']\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = .3)\n",
    "\n",
    "# fit\n",
    "mnb = nb.MultinomialNB(alpha = .4)\n",
    "mnb.fit(X_train, y_train)\n",
    "# score\n",
    "print(\"Accuracy:\", mnb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ab4486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17520)\t0.2193415827357477\n",
      "  (0, 16712)\t0.291102930020111\n",
      "  (0, 401)\t0.29279230041783566\n",
      "  (0, 10335)\t0.2599423583435001\n",
      "  (0, 455)\t0.23567602704432328\n",
      "  (0, 8047)\t0.335009226866536\n",
      "  (0, 3130)\t0.48977264047592894\n",
      "  (0, 3397)\t0.41387288873118766\n",
      "  (0, 19033)\t0.34724102010901836\n",
      "  (0, 14403)\t0.1197175347666778\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb249dad",
   "metadata": {},
   "source": [
    "## Results\n",
    "Tuning the model yielding roughly a 6% increase in accuracy, however, surely more work could be done on the text cleaning. Regardless, fairly promising results for minimal effort. Sure beats manual classification! I sure as hell wouldn't want to read even a fraction of the tweets in this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
